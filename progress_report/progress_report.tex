% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{physics}
\usepackage{cleveref}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Investigating Layer-Specific Vulnerability of LLMs to
Adversarial Attacks\\\normalfont{\large{Mid-term progress report}}}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Cagatay Gultekin\textsuperscript{1} \\
  (cgultekin) \\\And
  Fabio Giovanazzi\textsuperscript{1} \\
  (fgiovanazzi) \\\And
  Adam Rahmoun\textsuperscript{1} \\
  (arahmoun) \\\And
  Tobias Kaiser\textsuperscript{1} \\
  (tokaiser) \\\AND
\\
  \textsuperscript{1}ETH Zürich
}


% \author{Cagatay Gultekin\textsuperscript{1} \\
%   (cgultekin) \\
%   ETH Zürich \\\And
%   Fabio Giovanazzi\textsuperscript{1} \\
%   (fgiovanazzi) \\
%   ETH Zürich \\\And
%   Adam Rahmoun\textsuperscript{1} \\
%   (arahmoun) \\
%   ETH Zürich \\\And
%   Tobias Kaiser\textsuperscript{1} \\
%   (tokaiser) \\
%   ETH Zürich
% }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
This document explains the progress we have made on the project so far. We have studied the TinyLlama model internals and wrote a script to perform the training process based on pretrained weights found on HuggingFace and on the C4 dataset. We adapted the training procedure to freeze all layers except those we want to inspect and we included a gradient regularization term in the loss. Finally, we ran the GCG attack on the trained models, bumping into roadblocks due to the immense amount of training required and the limits of a small model like TinyLlama.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\Cref{sec:intro} summarizes the proposal we previously submitted and provides context, \cref{sec:training} and \cref{sec:attack} explain our progress respectively on the training setup and on the attack setup, \cref{sec:roadblocks} lists the issues we have faced so far and how we plan to overcome them, and finally \cref{sec:todos} lays out a plan to complete the project successfully.

\subsection{Summary of the proposal}

LLMs have demonstrated remarkable performance across tasks, however they remain vulnerable to adversarial ``jailbreaks'', such as \cite{zou2023universal}'s Greedy Coordinate Descent (GCG). GCG is a white-box attack that relies on gradients to efficiently search through token combinations that may confuse the model.

In the proposal of the project we set out to investigate which layers of an LLM are more influential in making the attack succeed. Since GCG relies on gradients, we realised that to analyze one (or a handful) of layers at a time, we could continue the training of a pre-trained (vulnerable) model with a gradient regularizer term in the loss to push gradients of those layers to zero. We considered two options: updating all of the weights, or freezing all layers other than the ones under analysis to prevent the model from changing its behavior significantly.

To ensure our training process would not significantly reduce the performance of the models, making the analysis worthless, we decided to keep the models robustness in check during our investigation, and to change our approach in case the performance did not remain stable. Furthermore, we planned to check whether the prompts generated by the attack would remain transferable to commercial models like ChatGPT, and see if there are patterns depending on which layers' gradients are pushed to 0.

\subsection{Setup}

We decided to work on the TinyLlama-1.1B-Chat-v1.0 model \cite{zhang2024TinyLlama}, given its manageable size and reasonable performance, but to also consider other models for comparison. To perform the additional training, we chose the C4 dataset presented in \cite{dodge-etal-2021-documenting}, and expected to use a 3-5GB subset of it. Finally, to evaluate the attack performance, we naturally picked AdvBench, the benchmark introduced by the authors of GCG in \cite{zou2023universal}. It consists of two primary components: 500 samples of Harmful Strings that aligned models should not generate, and 500 samples of Harmful Behaviors, i.e. prompts requesting the model to generate harmful content.

\subsection{Research questions}

\begin{itemize}
    \item[RQ1.] Which layers of an LLM contribute most significantly to adversarial vulnerability?
    \item[RQ2.] How does layer-specific gradient regularization affect overall robustness?
    \item[RQ3.] Are these layer sensitivity patterns consistent across architectures and scales?
    \item[RQ4.] Can these patterns help explain the high transferability to commercial models like ChatGPT?
\end{itemize}

\section{Training the model}
\label{sec:training}

The first task we focused on was setting up the training pipeline for the TinyLlama-1.1B-Chat-v1.0 model \cite{tinyllamahuggingface}. The resulting Python script supports various command line options to select the model, the hyperparameters, and other things.

\subsection{Model details}

We studied the model structure in the TinyLlama repository \cite{tinyllamarepo} and collected some information about it. The model supports 32000 different tokens that are turned into embedding vectors of size 2048. The embeddings are then passed through 22 layers, each made of an attention step inverted-bottleneck SwiGLU MLP. In order for the attention layers to be able to reason about the position of tokens, TinyLlama employs rotary embeddings. A customized Root Mean Square is used to normalize the outputs after each internal step. Finally, the results of the repeated layers are fed into a language model head that predicts the next token probabilities.

The parameters of the model are 16-bit floating point numbers, i.e. \texttt{torch.bfloat16}. This allows the model to use half of the space on disk and in RAM with respect to 32-bit floats, which is the float size used during training in \cite{tinyllamarepo}.

\subsection{Loading the model}

To perform the work planned for this project, we needed to load the model in a way that would allow accessing the single layers to freeze them or collect only some gradients. One of the group members initially planned to reimplement the model using standard \texttt{torch} building blocks based on the code in the repository \cite{tinyllamarepo}, thinking it would be needed to have enough flexibility. However, even after properly loading the correct weights into the layers, the model output probabilities did not match with the expected ones, likely due to a mismatch in the implementation details of the norm and of the rotary embeddings.

So we moved onto just loading the model using Python's \texttt{transformers} library, which, as it turned out, is very easy to use and does allow loading the \texttt{torch} model directly, satisfying our flexibility needs. Accessing specific layers can be achieved by looping through \texttt{model.named\_parameters()}.

\subsection{Loss function}

As explained in the project proposal, we decided to use the following loss to achieve gradients close to zero in the layer(s) under analysis:

\[ L_{\textrm{total}} = L_{\textrm{task}} + \lambda \sum_{i \in \mathcal{I}} \left\| \pdv{\textrm{Layer}_i(\textrm{input})}{\textrm{input}} \right\|_2^2 \]

where $\mathcal{I}$ is the set of indices of layers under analysis, and $L_{\textrm{task}}$ is the loss used to train the model originally, that is, cross-entropy loss. $\lambda$ is a hyperparameter that determines how aggressively the gradients are pushed to 0.

\subsection{Data and optimization}
\label{sec:dataopt}

We loaded the C4 dataset in a streaming fashion using the Python library \texttt{datasets}. Before the data could be passed to the model, it needed to be tokenized using TinyLlama's tokenizer.

To perform optimization, we decided to start by using plain Gradient Descent with tunable batch size $b$ and learning rate $\eta$, and switch to a more advanced optimizer later.

\subsection{Results}

Preliminary results after a couple hundred steps of gradient descent with $b=2$, $\eta=0.0004$ (as advised in \cite{tinyllamarepo}), $\lambda=0.01$ show that the gradients of the single layer under analysis are indeed pushed towards zero. Despite the gradient regularizer term, the model robustness remains good, or at least the model seems to provide outputs in line with those it provided before the additional training when interacting manually.

\section{Performing the attack}
\label{sec:attack}

We loaded the Greedy Coordinate Gradient (GCG) attack infrastructure from the author's official repository \cite{llmattacksrepo} and tried to reproduce their results on the unmodified TinyLlama-1.1B-Chat-v1.0 \cite{tinyllamahuggingface}.

The GCG attack iteratively performs these steps:

\begin{enumerate}
    \item it appends some tokens to a harmful base prompt taken from the AdvBench dataset \cite{zou2023universal}
    \item it computes the gradients of the loss with respect to those added tokens (considering the tokens as part of a continuous space of embeddings); the loss is lower the closer the model is to generating harmful strings
    \item for each position, it identifies top-$k$ alternative tokens that reduce the loss the most, by picking $k$ tokens with closeby embeddings in the direction of the gradient
    \item it greedily selects the best sequence of tokens among the top-$k$ alternatives for each token that minimizes the loss
\end{enumerate}

Therefore the GCG attack is effectively a training process specific to each prompt that operates on discrete parameters (i.e. the sequence of added tokens), and as such requires some time to run. In order to evaluate whether a model is vulnerable to GCG, the AdvBench dataset provides a set of 500 harmful prompts to run GCG on, and 500 harmful strings to check if a model response indicates that the attack succeeded. Therefore, evaluating a single model implies performing 500 training procedures, which is probably going to require a lot of time.

\section{Roadblocks}
\label{sec:roadblocks}

\subsection{Running out of RAM}

One issue we faced with the training process was that we couldn't train TinyLlama on more than $b=2$ batches, as they wouldn't fit in our computer's GPUs (e.g. NVIDIA GeForce RTX 3070 has only 8GB of RAM). Therefore in the future we are going to use either the cluster provided by ETH or some Google Colab Pro notebooks (though the cluster is preferred to run quick tests from CLI).

\subsection{TinyLlama not performant enough}

Another issue we are worried about is the fact that TinyLlama, with just 1.1B parameters, might be too small to be actually useful. The authors of the LLM attacks \cite{zou2023universal} used the Vicuna-7B and Vicuna-13B models \cite{zheng2023judgingllmasajudgemtbenchchatbot} instead, which rank much higher in benchmarks than TinyLlama \footnote{\href{https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\#/?search=tinyllama\%3Bvicuna\%3Bphi-2}{see Open LLM Leaderboard}}. However, considering that we need to perform a lot of training processes to complete this project, we decided to avoid using models as big as Vicuna 7B. A middle-ground alternative to try could be Microsoft's phi-2 \cite{javaheripi2023phi}, which has just 2.7B parameters and provides exceptional performance for its size \footnotemark[1].

\subsection{Too much training to do}

In the initial proposal we set out to train $\ell$ models, where $\ell$ is the number of layers (in the case of TinyLlama $\ell = 22$), because of the fact that we want to regularize the gradients of one layer at a time to see how much that layer affects the vulnerability of the model. However, we quickly realized that training such a big number of models is going to take quite some time (although 22 is still a manageable number). Moreover, as highlighted in \cref{sec:attack}, to evaluate the effectiveness of the attack on each of the $\ell$ models, we are going to need to run 500 (shorter) training procedures on each model, which is going to take a lot of time and resources. Therefore, we believe it would be better to reduce the amount of trainings to do, for example by only evaluating the models on a subset of the AdvBench dataset. Another solution we are going to employ is to analyze the impact of multiple layers at a time instead of every one separately, for example ``the early layers'', ``the middle layers'', or ``the final layers'', reducing the number of models from $\ell$ to $3$.

\section{Next steps}
\label{sec:todos}

To conclude this progress report, we list a few important TODOs we still need to work on:

\begin{itemize}
    \item we should try to tune $\lambda$ and determine if there are values that maintain the model robustness more than others while still pushing the gradients towards $0$
    \item we should use the optimizer used by the model authors instead of using plain Gradient Descent (see \cref{sec:dataopt})
    \item we should run benchmarks after training the models to ensure their performance is not affected by gradient regularization
\end{itemize}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\end{document}
